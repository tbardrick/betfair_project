{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Idea 2 : \"Cointegration - pairs trading - version 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes (to do)\n",
    "* Use lay prices as well (currently only using back prices, but two lay bets are made per pairs trade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0 : Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages\n",
    "from pathlib import Path, PurePath \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def payout(bp, bs, lp, ls, c = 0):\n",
    "    if ls == '?':\n",
    "        ls = lay_hedge_stake(bp, bs, lp, c)\n",
    "    elif bs == '?':\n",
    "        bs = bet_hedge_stake(lp, ls, bp, c)\n",
    "    loss_side = - bs + ls * (1 - c) \n",
    "    win_side = (bp - 1) * bs * (1 - c) - (lp - 1) * ls\n",
    "    return win_side, loss_side \n",
    "\n",
    "def lay_hedge_stake(bp, bs, lp, c):\n",
    "    return (((bp - 1) * bs * (1 - c)) + bs) / (lp)\n",
    "\n",
    "def bet_hedge_stake(lp, ls, bp, c):\n",
    "    return ls * (lp - c) / (bp * (1 - c) + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12906, 307)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SelectionId</th>\n",
       "      <th>MarketId</th>\n",
       "      <th>Venue</th>\n",
       "      <th>Distance</th>\n",
       "      <th>RaceType</th>\n",
       "      <th>BSP</th>\n",
       "      <th>NoRunners</th>\n",
       "      <th>BS:T-60</th>\n",
       "      <th>BS:T-59</th>\n",
       "      <th>BS:T-58</th>\n",
       "      <th>...</th>\n",
       "      <th>LS:T+5</th>\n",
       "      <th>LS:T+6</th>\n",
       "      <th>LS:T+7</th>\n",
       "      <th>LS:T+8</th>\n",
       "      <th>LS:T+9</th>\n",
       "      <th>LS:T+10</th>\n",
       "      <th>LS:T+11</th>\n",
       "      <th>LS:T+12</th>\n",
       "      <th>LS:T+13</th>\n",
       "      <th>LS:T+14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11688029.0</td>\n",
       "      <td>1.166898</td>\n",
       "      <td>Southwell</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>9.2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.15</td>\n",
       "      <td>5.98</td>\n",
       "      <td>6.86</td>\n",
       "      <td>...</td>\n",
       "      <td>4.76</td>\n",
       "      <td>7.70</td>\n",
       "      <td>3.07</td>\n",
       "      <td>41.07</td>\n",
       "      <td>8.05</td>\n",
       "      <td>3.74</td>\n",
       "      <td>1.85</td>\n",
       "      <td>7.05</td>\n",
       "      <td>3.89</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13331255.0</td>\n",
       "      <td>1.166898</td>\n",
       "      <td>Southwell</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Flat</td>\n",
       "      <td>4.3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>41.50</td>\n",
       "      <td>64.89</td>\n",
       "      <td>38.54</td>\n",
       "      <td>...</td>\n",
       "      <td>16.44</td>\n",
       "      <td>7.38</td>\n",
       "      <td>18.12</td>\n",
       "      <td>5.44</td>\n",
       "      <td>4.09</td>\n",
       "      <td>15.50</td>\n",
       "      <td>3.82</td>\n",
       "      <td>66.43</td>\n",
       "      <td>192.93</td>\n",
       "      <td>136.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SelectionId  MarketId      Venue  Distance RaceType  BSP  NoRunners  \\\n",
       "0   11688029.0  1.166898  Southwell       8.0     Flat  9.2        7.0   \n",
       "1   13331255.0  1.166898  Southwell       8.0     Flat  4.3        7.0   \n",
       "\n",
       "   BS:T-60  BS:T-59  BS:T-58  ...  LS:T+5  LS:T+6  LS:T+7  LS:T+8  LS:T+9  \\\n",
       "0     4.15     5.98     6.86  ...    4.76    7.70    3.07   41.07    8.05   \n",
       "1    41.50    64.89    38.54  ...   16.44    7.38   18.12    5.44    4.09   \n",
       "\n",
       "   LS:T+10  LS:T+11  LS:T+12  LS:T+13  LS:T+14  \n",
       "0     3.74     1.85     7.05     3.89     0.41  \n",
       "1    15.50     3.82    66.43   192.93   136.06  \n",
       "\n",
       "[2 rows x 307 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in data\n",
    "project_dir = Path.cwd().parents[2]\n",
    "data_dir = project_dir / 'data' / 'processed' / 'api' / 'advanced' / 'adv_data.csv'\n",
    "df = pd.read_csv(data_dir, header = 1, low_memory = False, index_col = 0)\n",
    "print(df.shape)\n",
    "\n",
    "# defining variables\n",
    "back_prices = [col for col in df.columns if 'BP' in col]\n",
    "back_sizes = [col for col in df.columns if 'BS' in col]\n",
    "lay_prices = [col for col in df.columns if 'LP' in col]\n",
    "lay_sizes = [col for col in df.columns if 'LS' in col]\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approach to pairs trading\n",
    "\n",
    "__2.0__ **- [Herlemont (2004)](http://docs.finance.free.fr/DOCS/Yats/cointegration-en%5B1%5D.pdf) paper**\n",
    "\n",
    "Herlemont describes in detail the econometrics of pairs trading for financial market assets. The following partly follows his commentary with some additional clarifications and discussion relating to horse racing.\n",
    "\n",
    "**2.1 - Testing for mean reversion**\n",
    "\n",
    "The aim is to identify odds that move together and whose spread is mean reverting. For the purposes of horse racing pairs, mean reversion is essential. Our objective is to capture prices whose spread has (temporarily) deviated from its mean. If this can be found, bets can be made to take advantage of the possible reversion.\n",
    "\n",
    "A stochastic process $y_{t}$ that is weakly stationary has the following properties for all $t$:\n",
    "\n",
    "* $E[y_{t}] = \\mu < \\infty$\n",
    "* $var(y_{t}) = \\gamma_{0} < \\infty$\n",
    "* $cov(y_{t}, y_{t-j}) = \\gamma_{j} < \\infty, j = 1, 2, 3 ...$\n",
    "\n",
    "(constant mean, constant variance, covariance between two observations depends only on the distance in time between them)\n",
    "\n",
    "A weakly stationary $I(0)$ series:\n",
    "* Fluctuates around its mean with a finite variance that does not depend upon time.\n",
    "* Is mean-reverting: it has tendency to return to its mean.\n",
    "* Has limited memory; the effect of a shock dies out. Autocorrelations die out (fairly) rapidly.\n",
    "\n",
    "With two horse's odds, $A_{t}$ and $B_{t}$, we look at $y_{t} = \\log \\frac{A_{t}}{B_{t}} = \\log A_{t} - \\log B_{t}$. This is once again the spread between the prices of the two horses, defined slightly differently. We want to find a pair which has a weakly stationary spread. We are interested in the ($AR(1)$) process \n",
    "\n",
    "$y_{t} = c + \\theta y_{t-1} + \\varepsilon_{t}$,\n",
    "\n",
    "or the log odds ratio over time. If this is weakly stationary, it would suggest a mean reverting process. \n",
    "\n",
    "The three previous conditions, and a stability condition that $|\\theta|<1$ (that the process $y_{t}$ is not a random walk or that it follows an eratic positive-to-negative pattern) must hold.\n",
    "______\n",
    "\n",
    "A Dickey-Fuller stationarity test can be carried out on the log ratio of the prices to test whether a process is weakly stationary. If we carry out the regression:\n",
    "\n",
    "$\\Delta y_{t} = \\mu + \\omega y_{t-1} + \\varepsilon_{t}$\n",
    "\n",
    "where the null hypothesis that $\\omega = 0$ is that the 'true' relationship is $\\Delta y_{t} = \\mu + \\varepsilon_{t} \\Leftrightarrow y_{t} = \\mu + y_{t-1} + \\varepsilon_{t}$, or a random walk with starting point $y_{0} = \\mu$.\n",
    "\n",
    "If we can reject the null hypothesis, the price ratio is weakly stationary and thereby mean-reverting.\n",
    "\n",
    "A Dickey-Fuller test is required for each possible pair of horses in a race, or $\\frac{n(n-1)}{2}$ regressions, where $n$ is the number of horses.\n",
    "\n",
    "While we are interested in the stochastic process $y_{t}$, we do not need to carry out the regression of $y_{t} = c + \\theta y_{t-1} + \\varepsilon_{t}$ for the purpose of finding pairs. This relationship between a pair of odds itself is not important to quantify. We are only interested in the features of the process. \n",
    "____\n",
    "\n",
    "*In the previous analysis, the test for whether two odds formed a pair was to find the pair with the smallest sum of absolute differences over time in the standardised prices. That method would allow maximum 1 pair to be found per race, and the validity of that pair would not be confirmed statisticallyather. Rather, the pair's feasibilty for a trade would be tested for afterwards based on profitability. I have more confidence in the approach in this section.*\n",
    "\n",
    "**2.2 - Screening pairs**\n",
    "\n",
    "Herlemont describes rules to ensure that market neutrality is more achievable in pairs trading. The idea is to pick stocks with very similar characteristics like same industry and similar market betas, with the intention of minimising asymmetric shocks to the price of one stock and not the other. For example in the case of two stocks, the share on which you are long is a business heavily dependent on oil, while the other share is not, a surge in oil prices which dampens profitability of your long share will likely see its price fall, ruining the pairs trade. In the case of shares, the simplest solution would be to pick shares in similar industries with similar market betas (or with similar idiosyncratic risks).\n",
    "\n",
    "For horses, the external factors influencing prices (news about runners, changing weather conditions, etc.) will usually always have asymmetric effects. This may be avoidable through picking horses with similar fundamental characteristics. However, this is very complicated. My hope is that the pair finding mechanism picks horses where this is already the case, because the market reacts the same way to news for these horse pairs.\n",
    "\n",
    "We cannot follow a beta-based approach because there are not 'market-wide fluctuations' of the same sort. However, there is the fact that the implied probability of all horses in the market book is equal to approximately 1. Therefore, you could say that for a given change in implied probability for one horse, the sum of the changes in the odds of all the remaining horses is the negative the change for the given horse:\n",
    "\n",
    "$\\Delta O_{i} = - \\sum_{j = 1, j \\neq i}^{N_{h}} \\Delta O_{j} $\n",
    "\n",
    "There is therefore interdependence between all prices across the market. It's possible that this will cause an endogeneity problem in regressions between separate horses, as the changes in the dependent variable necessarily impact the explanatory variable. However, the impact is likely to be very small, and will be smaller the greater the number of horses. \n",
    "\n",
    "*In Bebbington's analysis, he describes that betting £1 on one of the horses and £$\\beta$ on the other creates a market neutral bet. This is incorrect, and it appears that he has misunderstood hedging in this context. In that analysis, $\\beta = \\frac{y_{t}}{x_{t}}$, and therefore he is simply considering the ratio of the prices of the horses, the same ratio considered when determining the optimal stake for two given prices in a hedge. It is correct that on a single horse this creates a market neutral bet, however neutrality in horse racing means neutral to the outcome of the race. Any bet neutral to the race outcome is definitively neutral to the market. When betting on separate horses, the bets on each horse must be made neutral separately. Additionally, the use of $\\beta$ in staking is unneccesary. Consider the case where £$BS$ has been bet on horse A at price $BP$. Now, horse A is priced at $LP$. The optimal stake to bet on LP is £$LS = \\frac{BS * BP}{LP}$. In the aforementioned regression, $BS = 1$, hence $\\beta = \\frac{y_{t} * 1}{x_{t}}$ is the optimal stake only for bets of £1, otherwise it would be $S*\\beta$. More importantly, using the estimated $\\beta$ to find the an approximation of the optimal stake makes no sense when you can simply find the optimal stake with the aforementioned equation.*\n",
    "\n",
    "**2.3 - Trading rules**\n",
    "\n",
    "Timing rules must be added. \n",
    "\n",
    "Herlemont's basic rule is \"to open a position when the ratio of two share prices hits the 2 rolling standard deviation [difference from the 130-day rolling mean] and close it when the ratio returns to the mean.\"\n",
    "\n",
    "To avoid opening a position on stocks that are deviating from the mean and are going to deviate further, Herlemont describes that \"the position is not opened when the ratio breaks the two-standard-deviations limit for the first time, but rather when it crosses it to revert to the mean again.\"\n",
    "\n",
    "This can be achieved with the horse odds, of course in far smaller time scales. The current dataset is in 5-minute intervals for the three hours before a race; this should likely be expanded.\n",
    "\n",
    "Stop losses should be included and trade length should also be limited.\n",
    "\n",
    "Rules:\n",
    "1. Trade on pairs whose spread is reapproaching the mean from a deviated position\n",
    "2. Stop loss at x% of the initial position\n",
    "3. Don't hold open pairs trades for longer than x hours. \n",
    "\n",
    "It should be possible to quantify the average length of time required for a mean reversion and therefore the maximum logical time to hold open a position by looking at past data.\n",
    "\n",
    "**2.4 - Other tests and considerations**\n",
    "\n",
    "1. It should be ensured that the regression results of one price on another are not spurious (as with the regression in 2.5). $\\beta$ could be statistically meaningless if it is, meaning that it makes no sense to use it.\n",
    "2. I will also test whether $y_{t} = c + \\theta y_{t-1} + \\varepsilon_{t}$ is $I(1)$, or difference stationary. If we can rule this out, this gives more confidence in the 'weak-stationarity' of the spread over time.\n",
    "3. I will look out for $\\omega$ in the DF test that are close to 1 yet pass the DF test. They will have lots of features of a random walk, so the pairs exercise might be meaningless.\n",
    "4. Structural breaks (in this case, large instantaneous jumps in the spread) may make series that are stationary on either side of the break appear non-stationary. This is hard to account for in testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-e3538d566be3>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sample_df.drop_duplicates(inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h10715610</th>\n",
       "      <th>h12111722</th>\n",
       "      <th>h17574768</th>\n",
       "      <th>h19823</th>\n",
       "      <th>h21339426</th>\n",
       "      <th>h21822169</th>\n",
       "      <th>h23188356</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.090629</td>\n",
       "      <td>2.318458</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1.916923</td>\n",
       "      <td>2.089392</td>\n",
       "      <td>1.208960</td>\n",
       "      <td>1.745716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.068128</td>\n",
       "      <td>2.313525</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1.896119</td>\n",
       "      <td>2.104134</td>\n",
       "      <td>1.238374</td>\n",
       "      <td>1.738710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.071913</td>\n",
       "      <td>2.326302</td>\n",
       "      <td>2.944439</td>\n",
       "      <td>1.880991</td>\n",
       "      <td>2.104134</td>\n",
       "      <td>1.247032</td>\n",
       "      <td>1.729884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.051556</td>\n",
       "      <td>2.309561</td>\n",
       "      <td>2.904713</td>\n",
       "      <td>1.887070</td>\n",
       "      <td>2.104134</td>\n",
       "      <td>1.255616</td>\n",
       "      <td>1.740466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.028148</td>\n",
       "      <td>2.338917</td>\n",
       "      <td>2.862201</td>\n",
       "      <td>1.887070</td>\n",
       "      <td>2.104134</td>\n",
       "      <td>1.255616</td>\n",
       "      <td>1.738710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h10715610  h12111722  h17574768    h19823  h21339426  h21822169  h23188356\n",
       "0   2.090629   2.318458   2.944439  1.916923   2.089392   1.208960   1.745716\n",
       "1   2.068128   2.313525   2.944439  1.896119   2.104134   1.238374   1.738710\n",
       "2   2.071913   2.326302   2.944439  1.880991   2.104134   1.247032   1.729884\n",
       "3   2.051556   2.309561   2.904713  1.887070   2.104134   1.255616   1.740466\n",
       "4   2.028148   2.338917   2.862201  1.887070   2.104134   1.255616   1.738710"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new sample\n",
    "sample_df = df[df['MarketId'] == df['MarketId'].sample(1).item()]\n",
    "sample_df.drop_duplicates(inplace=True)\n",
    "\n",
    "bp_df = sample_df[['SelectionId'] + back_prices].copy()\n",
    "new_cols = bp_df.columns.str.replace(\"[BP:T]\", \"\").str.replace(\"[+]\", \"\")\n",
    "bp_df.rename(columns = dict(zip(bp_df.columns, new_cols)), inplace = True)\n",
    "bp_t_df = bp_df.T.copy()\n",
    "bp_t_df.columns = [\"h\" + str(int(column)) for column in bp_t_df.iloc[0]]\n",
    "bp_t_df = bp_t_df.iloc[1:-15] # using the 60 pre-off price data points\n",
    "bp_t_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "lp_df = sample_df[['SelectionId'] + lay_prices].copy()\n",
    "new_cols = lp_df.columns.str.replace(\"[LP:T]\", \"\").str.replace(\"[+]\", \"\")\n",
    "lp_df.rename(columns = dict(zip(lp_df.columns, new_cols)), inplace = True)\n",
    "lp_t_df = lp_df.T.copy()\n",
    "lp_t_df.columns = [\"h\" + str(int(column)) for column in lp_t_df.iloc[0]] #rename columns to horse ids\n",
    "lp_t_df = lp_t_df.iloc[1:-15] #remove horse ids, remove inplay data\n",
    "lp_t_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#using non-standardised log price data\n",
    "log_bp = np.log(bp_t_df[:30]).copy()\n",
    "log_bp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h11238576_h15836858</th>\n",
       "      <th>h11238576_h18227448</th>\n",
       "      <th>h11238576_h18889993</th>\n",
       "      <th>h11238576_h24635004</th>\n",
       "      <th>h11238576_h6561475</th>\n",
       "      <th>h15836858_h18227448</th>\n",
       "      <th>h15836858_h18889993</th>\n",
       "      <th>h15836858_h24635004</th>\n",
       "      <th>h15836858_h6561475</th>\n",
       "      <th>h18227448_h18889993</th>\n",
       "      <th>h18227448_h24635004</th>\n",
       "      <th>h18227448_h6561475</th>\n",
       "      <th>h18889993_h24635004</th>\n",
       "      <th>h18889993_h6561475</th>\n",
       "      <th>h24635004_h6561475</th>\n",
       "      <th>const</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.444868</td>\n",
       "      <td>-2.430418</td>\n",
       "      <td>-2.302585</td>\n",
       "      <td>0.228842</td>\n",
       "      <td>-0.818979</td>\n",
       "      <td>-2.875286</td>\n",
       "      <td>-2.747453</td>\n",
       "      <td>-0.216026</td>\n",
       "      <td>-1.263846</td>\n",
       "      <td>0.127833</td>\n",
       "      <td>2.659260</td>\n",
       "      <td>1.611440</td>\n",
       "      <td>2.531427</td>\n",
       "      <td>1.483607</td>\n",
       "      <td>-1.047820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.426372</td>\n",
       "      <td>-2.441847</td>\n",
       "      <td>-2.314014</td>\n",
       "      <td>0.217413</td>\n",
       "      <td>-0.831409</td>\n",
       "      <td>-2.868219</td>\n",
       "      <td>-2.740386</td>\n",
       "      <td>-0.208959</td>\n",
       "      <td>-1.257781</td>\n",
       "      <td>0.127833</td>\n",
       "      <td>2.659260</td>\n",
       "      <td>1.610438</td>\n",
       "      <td>2.531427</td>\n",
       "      <td>1.482605</td>\n",
       "      <td>-1.048822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.404295</td>\n",
       "      <td>-2.460409</td>\n",
       "      <td>-2.332576</td>\n",
       "      <td>0.195998</td>\n",
       "      <td>-0.850971</td>\n",
       "      <td>-2.864704</td>\n",
       "      <td>-2.736871</td>\n",
       "      <td>-0.208297</td>\n",
       "      <td>-1.255266</td>\n",
       "      <td>0.127833</td>\n",
       "      <td>2.656407</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>2.528574</td>\n",
       "      <td>1.481605</td>\n",
       "      <td>-1.046969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.377294</td>\n",
       "      <td>-2.476938</td>\n",
       "      <td>-2.357479</td>\n",
       "      <td>0.179468</td>\n",
       "      <td>-0.867501</td>\n",
       "      <td>-2.854233</td>\n",
       "      <td>-2.734773</td>\n",
       "      <td>-0.197826</td>\n",
       "      <td>-1.244795</td>\n",
       "      <td>0.119459</td>\n",
       "      <td>2.656407</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>2.536948</td>\n",
       "      <td>1.489978</td>\n",
       "      <td>-1.046969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.366931</td>\n",
       "      <td>-2.476938</td>\n",
       "      <td>-2.363770</td>\n",
       "      <td>0.185183</td>\n",
       "      <td>-0.851371</td>\n",
       "      <td>-2.843870</td>\n",
       "      <td>-2.730701</td>\n",
       "      <td>-0.181749</td>\n",
       "      <td>-1.218303</td>\n",
       "      <td>0.113169</td>\n",
       "      <td>2.662121</td>\n",
       "      <td>1.625567</td>\n",
       "      <td>2.548953</td>\n",
       "      <td>1.512399</td>\n",
       "      <td>-1.036554</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   h11238576_h15836858  h11238576_h18227448  h11238576_h18889993  \\\n",
       "0             0.444868            -2.430418            -2.302585   \n",
       "1             0.426372            -2.441847            -2.314014   \n",
       "2             0.404295            -2.460409            -2.332576   \n",
       "3             0.377294            -2.476938            -2.357479   \n",
       "4             0.366931            -2.476938            -2.363770   \n",
       "\n",
       "   h11238576_h24635004  h11238576_h6561475  h15836858_h18227448  \\\n",
       "0             0.228842           -0.818979            -2.875286   \n",
       "1             0.217413           -0.831409            -2.868219   \n",
       "2             0.195998           -0.850971            -2.864704   \n",
       "3             0.179468           -0.867501            -2.854233   \n",
       "4             0.185183           -0.851371            -2.843870   \n",
       "\n",
       "   h15836858_h18889993  h15836858_h24635004  h15836858_h6561475  \\\n",
       "0            -2.747453            -0.216026           -1.263846   \n",
       "1            -2.740386            -0.208959           -1.257781   \n",
       "2            -2.736871            -0.208297           -1.255266   \n",
       "3            -2.734773            -0.197826           -1.244795   \n",
       "4            -2.730701            -0.181749           -1.218303   \n",
       "\n",
       "   h18227448_h18889993  h18227448_h24635004  h18227448_h6561475  \\\n",
       "0             0.127833             2.659260            1.611440   \n",
       "1             0.127833             2.659260            1.610438   \n",
       "2             0.127833             2.656407            1.609438   \n",
       "3             0.119459             2.656407            1.609438   \n",
       "4             0.113169             2.662121            1.625567   \n",
       "\n",
       "   h18889993_h24635004  h18889993_h6561475  h24635004_h6561475  const  \n",
       "0             2.531427            1.483607           -1.047820      1  \n",
       "1             2.531427            1.482605           -1.048822      1  \n",
       "2             2.528574            1.481605           -1.046969      1  \n",
       "3             2.536948            1.489978           -1.046969      1  \n",
       "4             2.548953            1.512399           -1.036554      1  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dataframe where each column is log(horse a's prices) - log(horse b's prices). one new column for all n(n-1)/2 possible pairs\n",
    "\n",
    "#use itertools to find all possible comination pairs\n",
    "combos = list(itertools.combinations(log_bp.columns, 2))\n",
    "\n",
    "#creating dataframe\n",
    "for pair in combos:\n",
    "    if pair == combos[0]:\n",
    "        new_series = log_bp[pair[0]] - log_bp[pair[1]]\n",
    "        dickey_fuller_df = pd.DataFrame(new_series)\n",
    "    else:\n",
    "        new_series = log_bp[pair[0]] - log_bp[pair[1]]\n",
    "        dickey_fuller_df = pd.concat([dickey_fuller_df, new_series], axis=1)\n",
    "        \n",
    "#naming columns\n",
    "dickey_fuller_df.columns = [pair[0] + \"_\" + pair[1] for pair in combos]\n",
    "\n",
    "dickey_fuller_df['const'] = 1\n",
    "\n",
    "dickey_fuller_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair</th>\n",
       "      <th>coef</th>\n",
       "      <th>critical_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>h11238576_h15836858</td>\n",
       "      <td>-0.131551</td>\n",
       "      <td>-2.892398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>h11238576_h18227448</td>\n",
       "      <td>-0.113261</td>\n",
       "      <td>-1.519496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>h11238576_h18889993</td>\n",
       "      <td>-0.134122</td>\n",
       "      <td>-2.038246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h11238576_h24635004</td>\n",
       "      <td>-0.024029</td>\n",
       "      <td>-0.653254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>h11238576_h6561475</td>\n",
       "      <td>-0.206256</td>\n",
       "      <td>-1.636178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>h15836858_h18227448</td>\n",
       "      <td>-0.143266</td>\n",
       "      <td>-1.376951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>h15836858_h18889993</td>\n",
       "      <td>-0.275742</td>\n",
       "      <td>-2.050797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>h15836858_h24635004</td>\n",
       "      <td>-0.041601</td>\n",
       "      <td>-0.604386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>h15836858_h6561475</td>\n",
       "      <td>-0.082666</td>\n",
       "      <td>-1.712969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>h18227448_h18889993</td>\n",
       "      <td>-0.173770</td>\n",
       "      <td>-1.602845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>h18227448_h24635004</td>\n",
       "      <td>-0.162397</td>\n",
       "      <td>-1.364658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>h18227448_h6561475</td>\n",
       "      <td>-0.092048</td>\n",
       "      <td>-1.221973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>h18889993_h24635004</td>\n",
       "      <td>-0.068354</td>\n",
       "      <td>-0.737249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>h18889993_h6561475</td>\n",
       "      <td>-0.093098</td>\n",
       "      <td>-1.304852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>h24635004_h6561475</td>\n",
       "      <td>-0.029445</td>\n",
       "      <td>-0.726776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pair      coef  critical_value\n",
       "0   h11238576_h15836858 -0.131551       -2.892398\n",
       "1   h11238576_h18227448 -0.113261       -1.519496\n",
       "2   h11238576_h18889993 -0.134122       -2.038246\n",
       "3   h11238576_h24635004 -0.024029       -0.653254\n",
       "4    h11238576_h6561475 -0.206256       -1.636178\n",
       "5   h15836858_h18227448 -0.143266       -1.376951\n",
       "6   h15836858_h18889993 -0.275742       -2.050797\n",
       "7   h15836858_h24635004 -0.041601       -0.604386\n",
       "8    h15836858_h6561475 -0.082666       -1.712969\n",
       "9   h18227448_h18889993 -0.173770       -1.602845\n",
       "10  h18227448_h24635004 -0.162397       -1.364658\n",
       "11   h18227448_h6561475 -0.092048       -1.221973\n",
       "12  h18889993_h24635004 -0.068354       -0.737249\n",
       "13   h18889993_h6561475 -0.093098       -1.304852\n",
       "14   h24635004_h6561475 -0.029445       -0.726776"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dickey fuller test on each column\n",
    "\n",
    "#regression fit and results in vertical dataframe format (column for pair id, column for dickey fuller test result)\n",
    "dickey_fuller_results = {'pair' : [], 'coef' : [], 'critical_value' : []}\n",
    "\n",
    "for column in dickey_fuller_df:\n",
    "    if column == 'const':\n",
    "        break\n",
    "    reg = sm.OLS(endog = dickey_fuller_df[column].diff(), exog = dickey_fuller_df[['const', column]].shift(1), missing = 'drop')\n",
    "    results = reg.fit()\n",
    "    dickey_fuller_results['pair'].append(column)\n",
    "    dickey_fuller_results['coef'].append(results.params[1])\n",
    "    dickey_fuller_results['critical_value'].append(results.tvalues[1])\n",
    "\n",
    "dickey_fuller_results_df = pd.DataFrame(dickey_fuller_results)\n",
    "\n",
    "dickey_fuller_results_df\n",
    "\n",
    "#compare to -3.58 from the MacKinnon tables for 1% significance level, -2.93 for 5% significance level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to work out what the direction of the trade should be\n",
    "#the spreads are always defined as horse_a - horse_b, so if the spread > average, btl horse_a and ltb horse_b. if below average, vice versa.\n",
    "\n",
    "\n",
    "def bet(idx_open, time_open):\n",
    "    if (pair_df['spread'].iloc[open_trade_idx] > pair_spread_mean) and (pair_spread_mean > 0) or (pair_df['spread'].iloc[open_trade_idx] > pair_spread_mean) and (pair_spread_mean < 0):\n",
    "        #back to lay A (short)\n",
    "        bp_a = pair_df[horse_a + \"_bp\"].iloc[idx_open]\n",
    "        lp_a = pair_df[horse_a + \"_lp\"].iloc[idx_open + time_open]\n",
    "\n",
    "        win_side_a, loss_side_a = payout(bp_a, 1, lp_a, '?')\n",
    "        \n",
    "        #lay to back X (long)\n",
    "        lp_b = pair_df[horse_b + \"_lp\"].iloc[idx_open]\n",
    "        bp_b = pair_df[horse_b + \"_bp\"].iloc[idx_open + time_open]\n",
    "\n",
    "        win_side_b, loss_side_b = payout(bp_b, '?', lp_b, 1)\n",
    "\n",
    "        return win_side_a, win_side_b\n",
    "\n",
    "    elif (pair_df['spread'].iloc[open_trade_idx] < pair_spread_mean) and (pair_spread_mean < 0) or (pair_df['spread'].iloc[open_trade_idx] < pair_spread_mean) and (pair_spread_mean > 0): \n",
    "        #lay to back A (long)\n",
    "        lp_a = pair_df[horse_a + \"_lp\"].iloc[idx_open]\n",
    "        bp_a = pair_df[horse_a + \"_bp\"].iloc[idx_open + time_open]\n",
    "\n",
    "        win_side_a, loss_side_a = payout(bp_a, '?', lp_a, 1)\n",
    "\n",
    "        #back to lay A (short)\n",
    "        bp_b = pair_df[horse_b + \"_bp\"].iloc[idx_open]\n",
    "        lp_b = pair_df[horse_b + \"_lp\"].iloc[idx_open + time_open]\n",
    "\n",
    "        win_side_b, loss_side_b = payout(bp_b, 1, lp_b, '?')\n",
    "\n",
    "        return win_side_a, win_side_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below finds all possible pairs, finds where they have drifted 2sd from the mean and bets sequentially on all possible opportunities across all pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pairs.\n"
     ]
    }
   ],
   "source": [
    "#any pairs with a critical value less than have the null hypothesis rejected at the 1% significance level\n",
    "if dickey_fuller_results_df['critical_value'].min() < - 3.58:\n",
    "    \n",
    "    pairs_df = dickey_fuller_results_df.loc[dickey_fuller_results_df['critical_value'] < - 3.58].copy() #all possible pairs\n",
    "    print(f\"{len(pairs_df.index)} pair(s) found\\n\")\n",
    "    \n",
    "    for id_id in pairs_df['pair']:\n",
    "        pair_index = pairs_df.index[pairs_df['pair'] == id_id].item() #the most stationary pair \n",
    "        pair_cv = pairs_df['critical_value'].loc[pair_index]\n",
    "        pair_ids = pairs_df['pair'].loc[pair_index]\n",
    "        pair_coef = pairs_df['coef'].loc[pair_index]\n",
    "\n",
    "        horse_a = pair_ids.split(\"_\", 1)[0]\n",
    "        horse_b = pair_ids.split(\"_\", 1)[1]\n",
    "        pair_df = bp_t_df[[horse_a, horse_b]]\n",
    "        pair_df = pd.concat([pair_df, lp_t_df[[horse_a, horse_b]]], axis=1)\n",
    "        pair_df.columns = [horse_a + \"_bp\", horse_b + \"_bp\", horse_a + \"_lp\", horse_b + \"_lp\"]\n",
    "\n",
    "        #the following are all defined only in terms of BP\n",
    "        pair_df['spread'] = pair_df[horse_a + \"_bp\"] - pair_df[horse_b + \"_bp\"]\n",
    "\n",
    "        pair_spread_sd = np.std(pair_df['spread'][0:29], ddof = 1)\n",
    "        pair_spread_mean = pair_df['spread'][0:29].mean()\n",
    "\n",
    "        pair_df['deviation_2sd'] = np.where(abs(pair_df['spread']) - abs(pair_spread_mean) > 2 * pair_spread_sd, True, False)\n",
    "        pair_df['deviation_1sd'] = np.where(abs(pair_df['spread']) - abs(pair_spread_mean) > pair_spread_sd, True, False)\n",
    "\n",
    "        print(\"Pair row index: \" + str(pair_index), \", Pair ids: \" + str(pair_ids), \", Pair DF test critical value: \" + str(pair_cv), \", Pair theta: \" + str(pair_coef))\n",
    "        print(\"Pair average spread: \" + str(pair_spread_mean), \", Pair spread standard deviation: \" + str(pair_spread_sd) + \"\\n\")\n",
    "\n",
    "        open_trade_df = pair_df[pair_df['deviation_2sd'] == True].loc[30:] #so that we only consider data after the first 30 periods\n",
    "        \n",
    "        k = 5    \n",
    "        \n",
    "        while (len(open_trade_df.index) > 0) and ((open_trade_df.index[0] + k) < 59): #while len(open_trade_df.index) > k + 1:\n",
    "            print(open_trade_df.index[0])\n",
    "            open_trade_idx = open_trade_df.index[0] \n",
    "            win_side_a, win_side_b = bet(open_trade_idx, k)\n",
    "            open_trade_df = open_trade_df.loc[open_trade_idx + k + 1:]\n",
    "            \n",
    "            print(f\"Horse A payoff = {win_side_a}.\")\n",
    "            print(f\"Horse B payoff = {win_side_b}.\")\n",
    "    \n",
    "        else: print(\"No more trades.\\n\")\n",
    "    \n",
    "else: print(\"No pairs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo simulation**\n",
    "\n",
    "n repetitions of the above with profit summed over all trades.\n",
    "\n",
    "Trading rules:\n",
    "* Open trades when the price is between 2 and 4 standard deviations from the mean\n",
    "* Close trades k 2-minute periods later\n",
    "* Only consider races where 5 or less pairs are found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 of 500 iterations completed.\n",
      "200 of 500 iterations completed.\n",
      "300 of 500 iterations completed.\n",
      "400 of 500 iterations completed.\n",
      "500 of 500 iterations completed.\n",
      "Profit over 500 random race markets = -23.411116867011128. 308 pairs found, 253 pairs traded and 185 pairs trades made. 54 of 185 were profitable.\n"
     ]
    }
   ],
   "source": [
    "#setup\n",
    "n = 500 #number of iterations\n",
    "k = 10 #number of 2-minute periods trade is kept open (i.e. time expected for mean reversion to occur). this is use in the bet() function below\n",
    "\n",
    "#results variables\n",
    "profit = 0\n",
    "num_pairs = 0\n",
    "pairs_traded = 0\n",
    "num_trades_total = 0\n",
    "\n",
    "results_dict = {'pair' : [], 'mean_spread' : [], 'final_spread' : [], 'pair_cv' : [], 'pair_coef' : [], 'pairs_in_race' : [],\n",
    "                'num_trades' : [], 'profitable_trades' : [], 'losing_trades' : [], 'pc_trades_prof' : [],\n",
    "                'pair_profit' : [], 'pair_profits_list' : []}\n",
    "\n",
    "for i in range(n):\n",
    "            \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"{i+1} of {n} iterations completed.\")\n",
    "    \n",
    "    #new sample\n",
    "    sample_df = df[df['MarketId'] == df['MarketId'].sample(1).item()]\n",
    "    sample_df.drop_duplicates(inplace=True)\n",
    "\n",
    "    bp_df = sample_df[['SelectionId'] + back_prices].copy()\n",
    "    new_cols = bp_df.columns.str.replace(\"[BP:T]\", \"\").str.replace(\"[+]\", \"\")\n",
    "    bp_df.rename(columns = dict(zip(bp_df.columns, new_cols)), inplace = True)\n",
    "    bp_t_df = bp_df.T.copy()\n",
    "    bp_t_df.columns = [\"h\" + str(int(column)) for column in bp_t_df.iloc[0]]\n",
    "    bp_t_df = bp_t_df.iloc[1:-15] # using the 60 pre-off price data points\n",
    "    bp_t_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    lp_df = sample_df[['SelectionId'] + lay_prices].copy()\n",
    "    new_cols = lp_df.columns.str.replace(\"[LP:T]\", \"\").str.replace(\"[+]\", \"\")\n",
    "    lp_df.rename(columns = dict(zip(lp_df.columns, new_cols)), inplace = True)\n",
    "    lp_t_df = lp_df.T.copy()\n",
    "    lp_t_df.columns = [\"h\" + str(int(column)) for column in lp_t_df.iloc[0]] #rename columns to horse ids\n",
    "    lp_t_df = lp_t_df.iloc[1:-15] #remove horse ids, remove inplay data\n",
    "    lp_t_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    #using non-standardised log price data\n",
    "    log_bp = np.log(bp_t_df[:30]).copy()\n",
    "    log_bp.head()\n",
    "\n",
    "    #create dataframe where each column is log(horse a's prices) - log(horse b's prices). one new column for all n(n-1)/2 possible pairs\n",
    "    #use itertools to find all possible comination pairs\n",
    "    combos = list(itertools.combinations(log_bp.columns, 2))\n",
    "\n",
    "    for pair in combos:\n",
    "        if pair == combos[0]:\n",
    "            new_series = log_bp[pair[0]] - log_bp[pair[1]]\n",
    "            dickey_fuller_df = pd.DataFrame(new_series)\n",
    "        else:\n",
    "            new_series = log_bp[pair[0]] - log_bp[pair[1]]\n",
    "            dickey_fuller_df = pd.concat([dickey_fuller_df, new_series], axis=1)\n",
    "\n",
    "    #naming columns\n",
    "    dickey_fuller_df.columns = [pair[0] + \"_\" + pair[1] for pair in combos]\n",
    "    dickey_fuller_df['const'] = 1\n",
    "\n",
    "    #dickey fuller test on each column\n",
    "    dickey_fuller_results = {'pair' : [], 'coef' : [], 'critical_value' : []}\n",
    "\n",
    "    for column in dickey_fuller_df:\n",
    "        if column == 'const':\n",
    "            continue\n",
    "        reg = sm.OLS(endog = dickey_fuller_df[column].diff(), exog = dickey_fuller_df[['const', column]].shift(1), missing = 'drop')\n",
    "        results = reg.fit()\n",
    "        dickey_fuller_results['pair'].append(column)\n",
    "        dickey_fuller_results['coef'].append(results.params[1])\n",
    "        dickey_fuller_results['critical_value'].append(results.tvalues[1])\n",
    "\n",
    "    dickey_fuller_results_df = pd.DataFrame(dickey_fuller_results)\n",
    "\n",
    "\n",
    "    #continue if there is at least one 'stationary' pair\n",
    "    if dickey_fuller_results_df['critical_value'].min() < - 3.58:\n",
    "    \n",
    "        pairs_df = dickey_fuller_results_df.loc[dickey_fuller_results_df['critical_value'] < - 3.58].copy() #all possible pairs\n",
    "        \n",
    "        if len(pairs_df.index) < 6:  #TRADING RULE: REJECT RACES WHERE MORE THAN 5 PAIRS ARE FOUND\n",
    "            for id_id in pairs_df['pair']:\n",
    "\n",
    "                #pair analysis\n",
    "                pair_index = pairs_df.index[pairs_df['pair'] == id_id].item() #the most stationary pair \n",
    "                pair_cv = pairs_df['critical_value'].loc[pair_index]\n",
    "                pair_ids = pairs_df['pair'].loc[pair_index]\n",
    "                pair_coef = pairs_df['coef'].loc[pair_index]\n",
    "\n",
    "                horse_a = pair_ids.split(\"_\", 1)[0]\n",
    "                horse_b = pair_ids.split(\"_\", 1)[1]\n",
    "                pair_df = bp_t_df[[horse_a, horse_b]]\n",
    "                pair_df = pd.concat([pair_df, lp_t_df[[horse_a, horse_b]]], axis=1)\n",
    "                pair_df.columns = [horse_a + \"_bp\", horse_b + \"_bp\", horse_a + \"_lp\", horse_b + \"_lp\"]\n",
    "\n",
    "                #the following are all defined only in terms of BP\n",
    "                pair_df['spread'] = pair_df[horse_a + \"_bp\"] - pair_df[horse_b + \"_bp\"]\n",
    "\n",
    "                pair_spread_sd = np.std(pair_df['spread'][0:29], ddof = 1)\n",
    "                pair_spread_mean = pair_df['spread'][0:29].mean()\n",
    "\n",
    "                pair_df['deviation_2sd'] = np.where(abs(pair_df['spread']) - abs(pair_spread_mean) > 2 * pair_spread_sd, True, False)\n",
    "                pair_df['deviation_4sd'] = np.where(abs(pair_df['spread']) - abs(pair_spread_mean) > 4 * pair_spread_sd, True, False)            \n",
    "\n",
    "                #ALL TRADES ARE MADE IN THE CODE BELOW\n",
    "                open_trade_df = pair_df[pair_df['deviation_2sd'] == True].loc[30:] #only data after the first 30 periods\n",
    "                open_trade_df.drop_duplicates(inplace=True)\n",
    "\n",
    "                if len(open_trade_df.index) > 0:\n",
    "                    pairs_traded += 1\n",
    "\n",
    "                num_trades_pair = 0\n",
    "                profitable_trades_pair = 0\n",
    "                losing_trades_pair = 0\n",
    "                pair_profit = 0\n",
    "                pair_profits_list = []\n",
    "\n",
    "                #if there are indexs at which to make trades and trades can be completed, cycle through them\n",
    "                #TRADING RULE: IGNORE HORSES WHO ARE TRADING WITH SPREAD OF 3 SD OR GREATER THAN MEAN (to avoid horses who have deviated too much)    \n",
    "                while (len(open_trade_df.index) > 0) and ((open_trade_df.index[0] + k) < 59) and (open_trade_df['deviation_4sd'].loc[open_trade_df.index[0]] == False):\n",
    "                    open_trade_idx = open_trade_df.index[0] \n",
    "\n",
    "                    win_side_a, win_side_b = bet(open_trade_idx, k)\n",
    "                    \n",
    "                    #aggregate stats\n",
    "                    profit += win_side_a + win_side_b\n",
    "                    \n",
    "                    num_trades_total += 1\n",
    "\n",
    "                    #removes traded line from open_trade_df\n",
    "                    #+ 1 period gap between trades on a given pair. at this point one could add in a block to trading if a loss occured in the previous trade\n",
    "                    #edit the line below to alter the repetition of trades\n",
    "                    open_trade_df = open_trade_df.loc[open_trade_idx + k + 1:] \n",
    "\n",
    "                    #pair stats\n",
    "                    num_trades_pair += 1\n",
    "\n",
    "                    if (win_side_a + win_side_b) > 0:\n",
    "                        profitable_trades_pair += 1\n",
    "                    else:\n",
    "                        losing_trades_pair += 1\n",
    "                        \n",
    "                    pair_profit += win_side_a + win_side_b\n",
    "                    pair_profits_list.append(round(win_side_a + win_side_b,2))\n",
    "                    \n",
    "\n",
    "                #stats\n",
    "                num_pairs += 1\n",
    "                results_dict['pair'].append(id_id)\n",
    "                results_dict['mean_spread'].append(pair_spread_mean)\n",
    "                results_dict['final_spread'].append(pair_df['spread'].loc[59])\n",
    "                results_dict['pair_cv'].append(pair_cv)\n",
    "                results_dict['pair_coef'].append(pair_coef)\n",
    "                results_dict['pairs_in_race'].append(len(pairs_df.index))\n",
    "                results_dict['num_trades'].append(num_trades_pair) \n",
    "                results_dict['profitable_trades'].append(profitable_trades_pair) \n",
    "                results_dict['losing_trades'].append(losing_trades_pair) \n",
    "                try: results_dict['pc_trades_prof'].append((profitable_trades_pair / num_trades_pair) * 100)\n",
    "                except: results_dict['pc_trades_prof'].append(0)\n",
    "                results_dict['pair_profit'].append(pair_profit) \n",
    "                results_dict['pair_profits_list'].append(pair_profits_list)\n",
    "                #average profit\n",
    "                #number of horses in race\n",
    "                \n",
    "    \n",
    "    else: continue #move on to next iteration if there are no stationary series  \n",
    "\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "profitable_trades_total = results_df['profitable_trades'].sum()\n",
    "        \n",
    "print(f\"Profit over {n} random race markets = {profit}. {num_pairs} pairs found, {pairs_traded} pairs traded and {num_trades_total} pairs trades made. {profitable_trades_total} of {num_trades_total} were profitable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2_df = results_df[results_df['num_trades'] > 0].copy()\n",
    "results_2_df.to_csv(data_dir.parents[0] / 'pairs_trade_results.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
