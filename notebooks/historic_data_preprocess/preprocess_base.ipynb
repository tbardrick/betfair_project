{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for the moment, uncomment out the parts you need to run. if first time running, run all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 : importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path, PurePath \n",
    "import glob\n",
    "\n",
    "import betfairlightweight\n",
    "from betfairlightweight import filters\n",
    "\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from bz2 import BZ2File \n",
    "\n",
    "from betfairlightweight import StreamListener\n",
    "from betfairlightweight.streaming.stream import MarketStream\n",
    "\n",
    "import data_utils\n",
    "from data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 : 'logging in'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<LoginResource>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_dir = Path.cwd().parents[1]\n",
    "logins_dir = project_dir / 'api_logins.json'\n",
    "\n",
    "with open(logins_dir) as f:\n",
    "    login_dict =  json.load(f)\n",
    "    \n",
    "trading = betfairlightweight.APIClient(username=login_dict['my_username'],\n",
    "                                       password=login_dict['my_password'],\n",
    "                                       app_key=login_dict['my_app_key'],\n",
    "                                       certs=login_dict['certs_path'])\n",
    "\n",
    "trading.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 : retrieving api data list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # returns list of 'data dictionaries'\n",
    "# data_dicts = trading.historic.get_my_data()\n",
    "\n",
    "# # calculate range of dates for advanced data\n",
    "# adv_range = [d['forDate'] for d in data_dicts if d['plan'] == 'Advanced Plan']\n",
    "\n",
    "# # find min date for adv_data\n",
    "# adv_min_date = datetime.datetime.strptime(min(adv_range), '%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "# # find max data for adv data\n",
    "# def last_day_of_month(any_day):\n",
    "#     next_month = any_day.replace(day=28) + datetime.timedelta(days=4)  \n",
    "#     return next_month - datetime.timedelta(days=next_month.day)\n",
    "\n",
    "# adv_max_temp = datetime.datetime.strptime(max(adv_range), '%Y-%m-%dT%H:%M:%S')\n",
    "# adv_max_date = last_day_of_month(adv_max_temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # list files within advanced data range (GB Data)\n",
    "# adv_file_list = trading.historic.get_file_list(\n",
    "#     \"Horse Racing\",\n",
    "#     \"Advanced Plan\",\n",
    "#     from_day=adv_min_date.day,\n",
    "#     from_month=adv_min_date.day,\n",
    "#     from_year=adv_min_date.year,\n",
    "#     to_day=adv_max_date.day,\n",
    "#     to_month=adv_max_date.month,\n",
    "#     to_year=adv_max_date.year,\n",
    "#     market_types_collection=[\"WIN\"],\n",
    "#     countries_collection=[\"GB\"],\n",
    "#     file_type_collection=[\"M\"],\n",
    "# )\n",
    "# print(\"No. items :\", len(adv_file_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 : downloading data\n",
    "DON'T interrupt this when downloading or file may come corrupted and cause issues when processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # where to store our advanced data\n",
    "raw_dir = project_dir / 'data' / 'raw' / 'api' / 'advanced'\n",
    "\n",
    "# # check if files have been downloaded already\n",
    "# # adv_all_files = [Path(f).name for f in adv_file_list] # all files to download\n",
    "# adv_downloaded_files = [Path(f).name for f in raw_dir.glob(\"*.bz2\")] # all files downloaded\n",
    "\n",
    "# # download files we dont have and writing uncompressed versions\n",
    "# for file in adv_file_list: # remove index for all files\n",
    "#     if Path(file).name not in adv_downloaded_files: \n",
    "#         print(file)\n",
    "#         download = trading.historic.download_file(file_path = file, store_directory = raw_dir)\n",
    "#         print(download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 : processing data\n",
    "\n",
    "decompressing and writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proceesing bz2 to text (try/except for thos files corrupted when downloaded, not many)\n",
    "# adv_extfile_dirs = []\n",
    "\n",
    "# for file in glob.glob(str(adv_dir) + '/*.bz2'): # change this?\n",
    "#     try:\n",
    "#         zipfile = BZ2File(file) # open the file\n",
    "#         data = zipfile.read() # get the decompressed data\n",
    "#         newfilepath = file.split('.bz2')[0] # removing the extension and saving without a filetype\n",
    "#         open(newfilepath, 'wb').write(data) # write an uncompressed file\n",
    "#         adv_extfile_dirs.append(newfilepath)\n",
    "#         zipfile.close()\n",
    "#     except OSError:\n",
    "#         print(\"File not processed : \", file)\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code in to then redownload these files that are corrupted??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 : 'streaming' / writing processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_files = glob.glob(str(raw_dir) + '/*[!.bz2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadict = {'Time': [],\n",
    "       'MarketId' : [],\n",
    "       'Status' : [],\n",
    "       'Inplay' : [], \n",
    "       'SelectionId' : [],\n",
    "       'LastPriceTraded' : [],\n",
    "       'TotalMatched' : [],\n",
    "       'BSP' : [],\n",
    "       'AdjFactor' :  [],\n",
    "       'RunnerStatus' : [],\n",
    "       'MktTotalMatched' : [],\n",
    "       'RaceInfo' : [],\n",
    "       'Venue' : [],\n",
    "       'BackSize': [],\n",
    "       'BackPrice': [],\n",
    "       'LayPrice' : [],\n",
    "       'LaySize' : []\n",
    "}\n",
    "\n",
    "d = datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoricalStream(MarketStream):\n",
    "    # create custom listener and stream\n",
    "\n",
    "    def _init_(self, listener):\n",
    "        super(HistoricalStream, self)._init_(listener)\n",
    "\n",
    "    def on_process(self, market_books):\n",
    "        for market_book in market_books:\n",
    "            for runner in market_book.runners:\n",
    "\n",
    "                datadict['Time'].append(market_book.publish_time)\n",
    "                datadict['MarketId'].append(float(market_book.market_id))\n",
    "                datadict['Status'].append(market_book.status)\n",
    "                datadict['Inplay'].append(market_book.inplay)\n",
    "                datadict['SelectionId'].append(runner.selection_id)\n",
    "                datadict['LastPriceTraded'].append(runner.last_price_traded)\n",
    "                datadict['TotalMatched'].append(runner.total_matched)\n",
    "                datadict['BSP'].append(runner.sp.actual_sp)\n",
    "                datadict['AdjFactor'].append(runner.adjustment_factor)\n",
    "                datadict['RunnerStatus'].append(runner.status)\n",
    "                datadict['MktTotalMatched'].append(market_book.total_matched)\n",
    "                datadict['RaceInfo'].append(market_book.market_definition.name)\n",
    "                datadict['Venue'].append(market_book.market_definition.venue)\n",
    "                \n",
    "#                 atb_size = [x.size for x in runner.ex.available_to_back]\n",
    "#                 datadict['BackSize'].append(atb_size)\n",
    "                atb_price = [x.price for x in runner.ex.available_to_back]\n",
    "                datadict['BackPrice'].append(atb_price)   \n",
    "                atl_price = [x.price for x in runner.ex.available_to_lay]\n",
    "                datadict['LayPrice'].append(atl_price)\n",
    "#                 atl_size = [x.size for x in runner.ex.available_to_lay]\n",
    "#                 datadict['LaySize'].append(atl_size)\n",
    "\n",
    "\n",
    "class HistoricalListener(StreamListener):\n",
    "    def _add_stream(self, unique_id, stream_type):\n",
    "        if stream_type == \"marketSubscription\":\n",
    "            return HistoricalStream(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listener = HistoricalListener(max_latency=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.169028429 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.169028429\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.168013705 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.168013705\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.169341238 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.169341238\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.168473844 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.168473844\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.167971949 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.167971949\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.169057140 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.169057140\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.169866063 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.169866063\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.169675383\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.166947635 (1) stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.166947635 (1)\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.167449067 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.167449067\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.170188269 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.170188269\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.170070363 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.170070363\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.168298570 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.168298570\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.167929665 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.167929665\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.170070707\n",
      "/Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.167134920 stream completed.\n",
      "File not processed :  /Users/tombardrick/Documents/projects/betfair/betfair_project/data/raw/api/advanced/1.167134920\n"
     ]
    }
   ],
   "source": [
    "final_df = pd.DataFrame()\n",
    "interim_dir = project_dir / 'data' / 'interim' / 'api' / 'advanced'\n",
    "\n",
    "T_PRE = 60\n",
    "T_POST = 15\n",
    "\n",
    "\n",
    "for file in processed_files:\n",
    "    try:\n",
    "        datadict = {'Time': [],\n",
    "           'MarketId' : [],\n",
    "           'Status' : [],\n",
    "           'Inplay' : [], \n",
    "           'SelectionId' : [],\n",
    "           'LastPriceTraded' : [],\n",
    "           'TotalMatched' : [],\n",
    "           'BSP' : [],\n",
    "           'AdjFactor' :  [],\n",
    "           'RunnerStatus' : [],\n",
    "           'MktTotalMatched' : [],\n",
    "           'RaceInfo' : [],\n",
    "           'Venue' : [],\n",
    "           'BackSize': [],\n",
    "           'BackPrice': [],\n",
    "           'LayPrice' : [],\n",
    "           'LaySize' : []\n",
    "        }\n",
    "\n",
    "\n",
    "        stream = trading.streaming.create_historical_stream(directory=file, listener=listener)\n",
    "        stream.start() \n",
    "        stream.stop()\n",
    "\n",
    "        print(str(file) + \" stream completed.\")\n",
    "\n",
    "        # create df\n",
    "        df = dict_to_df(datadict)\n",
    "\n",
    "        df = pre_filter_races(df);\n",
    "        df = extract_race_info(df);\n",
    "        df = create_timedif(df);\n",
    "        df = filter_timedif(df);\n",
    "        df = create_time_bins(df, T_PRE, T_POST);\n",
    "        df = runner_groupby(df, T_PRE, T_POST)\n",
    "\n",
    "        # write / read from here\n",
    "\n",
    "        final_df = pd.concat([final_df, df])\n",
    "        print('Rows added : ', len(df.index), 'Total rows : ', len(final_df.index))\n",
    "        del df\n",
    "        \n",
    "\n",
    "    except:\n",
    "        print(\"File not processed : \", file)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 : writing out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dir = project_dir / 'data' / 'processed' / 'api' / 'advanced'\n",
    "final_df.to_csv(str(processed_dir) + '/adv_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do\n",
    "# use multiindexing for columns for time points / prices/ sizes in future?\n",
    "# move .py version to src/data/historic\n",
    "# investigate nulls befre removal in filtering\n",
    "# define 'hard variables' with capitals e.g. directories\n",
    "\n",
    "# add doc strings to each function e.g. qcut\n",
    "# include irish/us racing also\n",
    "\n",
    "# add in other variables like no.runners, country, date, win/loss\n",
    "\n",
    "# add in conditions to make script 'run beackawards' e.g.\n",
    "# - if market name not in final_df file then stream - and add it in - call this updates?\n",
    "\n",
    "# then wat to do with unprocessed files, redownload these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
